{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ON9w2DEPm57O"
   },
   "source": [
    "# **通过子类化定义新层和模型**\n",
    "\n",
    "### **引入**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtbT_27r2wO8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPb3JeSpndgP"
   },
   "source": [
    "### `Layer`类：状态（权重）和一些计算的组合体\n",
    "\n",
    "`Layer`类是Keras的核心抽象类之一。层封装了状态（层的“权重”）和从输入到输出的转换（“调用”，即层的前向传递）。\n",
    "\n",
    "下面是一个密度连接的层。它具有一个状态：变量`w`和`b`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LA-cgHz8ppzW"
   },
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()  # 随机高斯初始化对象\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()  # 0初始化\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXXGwk8fszwg"
   },
   "source": [
    "你可以通过在某些张量输入上调用层来使用它，就像使用Python函数一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "soIpnlxwtPRx",
    "outputId": "d4ee3e5d-06ac-4a4d-de1b-a7fb2c1dfd8c"
   },
   "outputs": [],
   "source": [
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FMCQ2nc6uayu"
   },
   "source": [
    "请注意，权重`w`和`b`在设置为层的属性时会自动由层跟踪："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWGPjDOZupmZ"
   },
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFA-ysi5ur_F"
   },
   "source": [
    "请注意，你还可以使用一种更快的方法为层添加权重：`add_weight()`方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "wv152zmPu0bj",
    "outputId": "996da62e-ac40-4217-c417-5a28072ee408"
   },
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCxizdwlu_Oh"
   },
   "source": [
    "### **层可以拥有不可训练的权重**\n",
    "\n",
    "除了可训练的权重之外，你还可以向层添加不可训练的权重。在训练层时，不能再反向传播期间使用这类权重。\n",
    "\n",
    "以下是添加和使用不可训练的权重的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "bdpQtVGpva2S",
    "outputId": "d9df2a3b-3e53-471c-b8da-7ce2dc836b4b"
   },
   "outputs": [],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UemSCCkFwBlK"
   },
   "source": [
    "它是`layer.weights`的一部分，但被归类为不可训练的权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "-o5XZ-JWwIQ5",
    "outputId": "6b391b27-a217-42f8-de8f-8883de9e241d"
   },
   "outputs": [],
   "source": [
    "print(\"weights:\", len(my_sum.weights))\n",
    "print(\"non-trainable weights:\", len(my_sum.non_trainable_weights))\n",
    "\n",
    "# 不包含可训练权重:\n",
    "print(\"trainable_weights:\", my_sum.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xI2ws_h9x8HA"
   },
   "source": [
    "### 最佳实践：得到输入的形状后，再创建权重\n",
    "\n",
    "我们上面的`Linear`层采用了`input_dim`参数，用于计算`__init__()`中权重`w`和`b`的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVPgvH8ByaWf"
   },
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yhc50mNvyeoP"
   },
   "source": [
    "在许多情况下，你可能事先不知道输入的大小，并且你想在实例化层后的某个时间，得到输入大小后再创建权重。\n",
    "\n",
    "在Keras API中，建议你在层的`build(self，inputs_shape)`方法中创建层的权重。像如下示例这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wmf3cMhj7uMQ"
   },
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):  # 不知道输入大小\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQXbhp4O77UK"
   },
   "source": [
    "层的`__call__()`方法在首次调用时将自动执行`build`。现在你有了一个懒惰的层，因此更易于使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZLkZkw38T4u"
   },
   "outputs": [],
   "source": [
    "# 实例化时，我们不知道使用何种输入进行调用\n",
    "linear_layer = Linear(32)\n",
    "\n",
    "# 首次调用该层时，将动态创建该层的权重\n",
    "y = linear_layer(x)\n",
    "print(y)\n",
    "\n",
    "# 权值创建后，只能接收符合形状要求的输入\n",
    "# x2 = tf.ones((4, 3))\n",
    "# y2 = linear_layer(x2)\n",
    "# print(y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEV9B_VmVV5_"
   },
   "source": [
    "### 层可以递归组合\n",
    "\n",
    "如果将层实例作为另一个层的属性，则外层将会开始跟踪内层的权重。\n",
    "\n",
    "我们建议在`__init__()`方法中创建这样的子层（由于子层通常具有构建方法，因此将在外层构建时构建它们）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "f7K1RrRcWTjV",
    "outputId": "3d00d81f-cfe7-480e-9c91-4f4e55573e36"
   },
   "outputs": [],
   "source": [
    "# 我们将上面定义了“build”方法的Linear类一起使用。\n",
    "\n",
    "\n",
    "class MLPBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32)  # 层实例作为属性，输入大小未知\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3, 64)))  # 第一次调用创建权重\n",
    "print(\"weights:\", len(mlp.weights))\n",
    "print(\"trainable weights:\", len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lLZmlqNyW07v"
   },
   "source": [
    "### add_loss()方法\n",
    "\n",
    "编写层的`call()`方法时，可以创建损失张量，随后将在编写训练循环时使用。这可以通过调用`self.add_loss(value)`来实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nXidThTyXI7Y"
   },
   "outputs": [],
   "source": [
    "# 产生活动正则化损失的层\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))  # 每次调用记录和输入有关的损失\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2enzuUkqXaHY"
   },
   "source": [
    "这些损失（包括由所有内部层产生的损失）可以通过`layer.losses`进行检索。此属性在顶层的`__call__()`开始时重置，因此`layer.losses`始终包含在上一次正向传递过程中创建的损失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ePmpi66YfJZ"
   },
   "outputs": [],
   "source": [
    "class OuterLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0  # 由于从未调用过该层，因此尚未产生任何损失\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # 创建一个损失值\n",
    "\n",
    "# `layer.losses`在__call__开始时被重置\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # 这是上面调用中造成的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i48qixC7Z8HQ"
   },
   "source": [
    "此外，`loss`属性还包含为所有内层的权重创建的正则化损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5nXwo8OfaEO4",
    "outputId": "aa01f64f-df30-4e64-9fd1-af45e629a77b"
   },
   "outputs": [],
   "source": [
    "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayerWithKernelRegularizer, self).__init__()\n",
    "        self.dense = keras.layers.Dense(\n",
    "            32, kernel_regularizer=tf.keras.regularizers.l2(1e-3)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayerWithKernelRegularizer()\n",
    "_ = layer(tf.zeros((1, 1)))\n",
    "\n",
    "# 下面的值由`1e-3 * sum(layer.dense.kernel ** 2)`计算得来,\n",
    "# 即通过上面的`kernel_regularizer`创建的.\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3OQa16Raybi"
   },
   "source": [
    "在编写训练循环时应考虑这些损失，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OV1cWo7a1In"
   },
   "outputs": [],
   "source": [
    "# 实例化一个优化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# 遍历数据集的批量。\n",
    "for x_batch_train, y_batch_train in train_dataset:\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = layer(x_batch_train)  # 小批量的Logits\n",
    "    # 小批量的损失值\n",
    "    loss_value = loss_fn(y_batch_train, logits)\n",
    "    # 添加在前向传递中产生的额外损失：\n",
    "    loss_value += sum(model.losses)\n",
    "\n",
    "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfWyOIJ-b6fj"
   },
   "source": [
    "有关编写训练循环的详细指南，请参阅[从头开始编写训练循环的指南](https://tensorflow.google.cn/guide/keras/writing_a_training_loop_from_scratch/)。\n",
    "\n",
    "这些损失还可以通过`fit()`无缝工作（它们会自动求和并添加到主要损失中）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "2aeR2ytbcTQu",
    "outputId": "7b8f4604-d3a1-46fb-f303-13677690afd0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "outputs = ActivityRegularizationLayer()(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# 如果在`compile`中传递了损失，则将正则化损失添加到其中\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "outputs = ActivityRegularizationLayer()(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# 也可以不给`compile`传递损失\n",
    "# 因为模型已经在前向传递过程中调用`add_loss`添加了一个需要最小化的损失！\n",
    "model.compile(optimizer=\"adam\")\n",
    "model.fit(np.random.random((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RiRwBX-neHj3"
   },
   "source": [
    "### add_metric()方法\n",
    "\n",
    "与`add_loss()`类似，层还具有`add_metric()`方法，用于在训练过程中跟踪指标的变化平均值。\n",
    "\n",
    "考虑以下的示例：“LogisticEndpoint”层。它以预测和目标作为输入，通过`add_loss()`跟踪计算的损失，并通过`add_metric()`跟踪计算的精度标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvgqzVH0f0ui"
   },
   "outputs": [],
   "source": [
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        # 计算训练时的损失，并使用`self.add_loss()`将其添加到层\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # 记录精度指标，并使用`self.add_metric()`将其添加到层\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name=\"accuracy\")\n",
    "\n",
    "        # 返回预测的张量(即`.predict()`).\n",
    "        return tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLfgL7UfgeT8"
   },
   "source": [
    "可通过`layer.metrics`访问以这种方式跟踪的指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "r8PSNkY3eqSj",
    "outputId": "f12e9606-4892-4cfd-cc0f-169cb053227a"
   },
   "outputs": [],
   "source": [
    "layer = LogisticEndpoint()\n",
    "\n",
    "targets = tf.ones((2, 2))\n",
    "logits = tf.ones((2, 2))\n",
    "y = layer(targets, logits)\n",
    "\n",
    "print(\"layer.metrics:\", layer.metrics)\n",
    "print(\"layer.losses:\", layer.losses)\n",
    "print(\"current accuracy value:\", float(layer.metrics[0].result()))\n",
    "print(\"current loss value:\", float(layer.losses[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPdaGFpAg21U"
   },
   "source": [
    "就像`add_loss()`一样，这些指标也可以通过`fit()`进行跟踪："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "QXV4Va1Pg9X5",
    "outputId": "9c7c8f76-3107-4e84-b071-70f6037abc10"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)  # 将输入和目标作为参数\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")\n",
    "\n",
    "data = {\n",
    "    \"inputs\": np.random.random((3, 3)),\n",
    "    \"targets\": np.random.random((3, 10)),\n",
    "}\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7OdJkGTmhJuU"
   },
   "source": [
    "### 你可以选择性的启用层的序列化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEm5J-GghXCH"
   },
   "source": [
    "如果需要将自定义层作为函数式模型的一部分进行序列化，则可以选择实现`get_config()`方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gG6tkSP1hlcG",
    "outputId": "2819887f-51f0-4970-f6c4-d0e7407f982f"
   },
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"units\": self.units}\n",
    "\n",
    "\n",
    "# 现在，你可以通过from_config重新创建该层：\n",
    "layer = Linear(64)\n",
    "print(layer)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)\n",
    "print(new_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BnmXXdhliOC-",
    "outputId": "8772d5f8-0d54-45d4-fcce-a71ee4393e78"
   },
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "\n",
    "\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pQMozd5ib-1"
   },
   "source": [
    "如果需要更大的灵活性，从其配置反序列化层时，则可以重写`from_config()`类方法来实现。下面是`from_config()`的基本实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2ABPWq9jPEj"
   },
   "outputs": [],
   "source": [
    "def from_config(cls, config):\n",
    "  return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3eHonr-BjSAH"
   },
   "source": [
    "要了解有关序列化和保存的更多信息，请参阅有关[模型保存和序列化的完整指南](https://tensorflow.google.cn/guide/keras/save_and_serialize/)。\n",
    "\n",
    "### call()方法中特殊的训练参数\n",
    "\n",
    "某些层，尤其是`BatchNormalization`层和`Dropout`层，在训练和预测期间具有不同的行为。 对于此类层，标准做法是在`call()`方法中公开训练（boolean）参数。\n",
    "\n",
    "通过在`call()`中公开此参数，可以启用内置的训练和评估循环（例如`fit()`）以在训练和预测中正确使用该层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NEzpO5ij-BZ"
   },
   "outputs": [],
   "source": [
    "class CustomDropout(keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GvvhCnRHkMc8"
   },
   "source": [
    "### call()方法中特殊的掩码参数\n",
    "\n",
    "`call()`支持的另一个特殊参数是`mask`参数。\n",
    "\n",
    "你可以在所有Keras RNN层中找到它。掩码是布尔张量（输入中每个时间步有一个布尔值），用于在处理时间序列数据时跳过某些输入时间步。\n",
    "\n",
    "当先前的层生成`mask`时，Keras会自动将正确的`mask`参数传递给支持该层的`__call__()`。`mask`生成层是配置有`mask_zero = True`的“`Embedding`”层和“`Masking`”层。\n",
    "\n",
    "要了解有关masking以及如何编写masking-enabled的层的更多信息，请查看指南“[理解padding和masking](https://tensorflow.google.cn/guide/keras/masking_and_padding/)”。\n",
    "\n",
    "### Model类\n",
    "\n",
    "通常情况下，你将使用`Layer`类来定义内部计算块，并使用`Model`类来定义外部模型--你将训练的对象。\n",
    "\n",
    "例如，在**ResNet50**模型中，你将有几个子类化`Layer`的**ResNet**，以及一个`Model`则包含了整个**ResNet50**网络。\n",
    "\n",
    "`Model`类具有与`Layer`同样的API，但有以下区别：\n",
    "+ 它公开了内置的训练，评估和预测循环（`model.fit()`，`model.evaluate()`，`model.predict()`）。\n",
    "+ 它通过`model.layers`属性公开其内层的列表。\n",
    "+ 它公开了保存和序列化API（`save()`，`save_weights()`...）\n",
    "\n",
    "实际上，`Layer`类对应于我们在文献中所称的“层”（如“卷积层”或“递归层”）或“块”（如“ ResNet块”或“ Inception块”） ）。\n",
    "\n",
    "同时，`Model`类对应于文献中所谓的“模型”（如“深度学习模型”）或“网络”（如“深度神经网络”）。\n",
    "\n",
    "因此，如果你想知道“我应该使用`Layer`类还是`Model`类？”，请问自己：我需要在其上调用`fit()`吗？我需要在上面调用`save()`吗？如果需要，请选择`Model`。如果不需要（因为你的类只是更大系统中的一个块，或者是因为你自己编写训练和保存代码），请使用`Layer`。\n",
    "\n",
    "例如，我们可以以上面的mini-resnet示例为例，并使用它来构建一个模型，该模型可以通过`fit()`进行训练，并可以通过`save_weights()`保存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wz4GDDp-qDb-"
   },
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_1 = ResNetBlock()\n",
    "        self.block_2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "resnet = ResNet()\n",
    "dataset = ...\n",
    "resnet.fit(dataset, epochs=10)\n",
    "resnet.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "roF8dDCcqPW1"
   },
   "source": [
    "### 整合所有知识点：端到端的示例\n",
    "\n",
    "+ 到目前为止，你已经学到了以下内容：\n",
    "+ `Layer`封装状态（在`__init__()`或`build()`中创建）和一些计算（在`call()`中定义）\n",
    "+ 可以递归嵌套层以创建更大的新计算块\n",
    "+ 层可以通过`add_loss()`和`add_metric()`创建和跟踪损失（通常是正则化损失）以及指标\n",
    "+ 你要训练的外部容器是一个`Model`。模型就像一个`Layer`，但是增加了训练和序列化功能\n",
    "\n",
    "让我们将所有这些内容放到一个端到端的示例中：我们将实现一个变分自动编码器（VAE）。我们将用MNIST数字对其进行训练。\n",
    "\n",
    "我们的VAE将是`Model`的子类，通过`Layer`的子类嵌套构建的。它将具有正则化损失（KL散度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71W4AQ6TrlW1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"使用(z_mean，z_log_var)采样z，该向量编码一个数字。”\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"将MNIST数字映射到三元组(z_mean，z_log_var，z)\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"将编码的数字矢量z转换回可读的数字\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"将编码器和解码器整合到端到端模型以进行训练\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # 添加KL散度作为正则化损失.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDqwhZ5fv0ne"
   },
   "source": [
    "让我们在MNIST上编写一个简单的训练循环："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "colab_type": "code",
    "id": "SF5bdL9bv5t1",
    "outputId": "65f1a996-1585-4143-f1ed-f2856519ad96"
   },
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# 遍历epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "    # 遍历dataset的批量\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # 计算重建损失\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)  # 添加KLD正则化\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Im-_tYK4w9AN"
   },
   "source": [
    "请注意，由于VAE是`Model`的子类，所以它具有内置的训练循环。因此，你也可以像这样训练它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "k0EhlyDIxFb6",
    "outputId": "06f95afd-fc7b-40f9-c54c-a8ec17464065"
   },
   "outputs": [],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7iD5QiDxMPP"
   },
   "source": [
    "### **超越面向对象的开发：函数式API**\n",
    "\n",
    "前面的示例对你来说，是不是使用了太多的面向对象开发？你也可以使用函数式API构建模型。重要的是，不论选择哪种风格不会妨碍你利用以另一种风格编写的组件：你始终可以混合搭配使用。\n",
    "\n",
    "例如，下面的函数式API示例中，重用了我们在上面示例中定义的相同`Sampling`层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "FfwRGeCJyCCC",
    "outputId": "7a8f0789-dfa6-4976-bf32-d85adf065012"
   },
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# 定义编码模型\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# 定义解码模型\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# 定义VAE模型\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# 添加KL散度为正则化损失\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# 训练\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfPAt3qzyF_5"
   },
   "source": [
    "有关更多信息，请确保阅读[函数式API指南](https://tensorflow.google.cn/guide/keras/functional/)。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNrrrFgzrZLzLYWXI31hGSe",
   "collapsed_sections": [],
   "name": "编写自定义层和模型.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
