{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARLMtZvVdeF6"
   },
   "source": [
    "# 高级自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lly9INbAdixY"
   },
   "source": [
    "[自动微分指南]包括了梯度计算的所有知识。本指南重点介绍tf.GradientTape 接口更深入，不常见的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-gyf1ufdoRh"
   },
   "source": [
    "## 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WuuVlaIjfABp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6BsjKxtzfIPR"
   },
   "source": [
    "## 控制梯度记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZk4RQ5tfX6y"
   },
   "source": [
    "在[自动微分指南]中，你已学会在进行梯度计算时，如何控制tape所监视的变量和张量。\n",
    "如果你想停止记录梯度的话，你可以用GradientTape.stop_recording() 去暂停记录。\n",
    "如果你不想在模型中对复杂的操作求微分，这方法可能会帮到你。它还可以计算指标或中间结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ad3G0ePMf74q"
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "  x_sq = x * x\n",
    "  with t.stop_recording():\n",
    "    y_sq = y * y\n",
    "  z = x_sq + y_sq\n",
    "\n",
    "grad = t.gradient(z, {'x': x, 'y': y})\n",
    "\n",
    "print('dz/dx:', grad['x'])  # 2*x => 4\n",
    "print('dz/dy:', grad['y'])  # 没有对y的微分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pNLU1_kgQ0B"
   },
   "source": [
    "如果你希望全部重新开始，请使用reset（）。通常，退出tape区域并重新开始会更容易阅读，但是当退出十分困难或根本不可能时，可以使用reset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qztAexsg_CI"
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "reset = True\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "  y_sq = y * y\n",
    "  if reset:\n",
    "    # 丢弃梯度带目前的所有记录\n",
    "    t.reset()\n",
    "  z = x * x + y_sq  # 再次记录\n",
    "\n",
    "grad = t.gradient(z, {'x': x, 'y': y})\n",
    "\n",
    "print('dz/dx:', grad['x'])  # 2*x => 4\n",
    "print('dz/dy:', grad['y'])  # 没有记录\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v20rSy9JhEJr"
   },
   "source": [
    "## 停止梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVVkB6lKhHVp"
   },
   "source": [
    "与上面对全局tape的控制不同，tf.stop_gradient函数要精确许多，它可阻止梯度沿特定的路径流动，而且无需访问tape本身："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lU7wZYYEhq6Z"
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "  y_sq = y**2\n",
    "  z = x**2 + tf.stop_gradient(y_sq)\n",
    "\n",
    "grad = t.gradient(z, {'x': x, 'y': y})\n",
    "\n",
    "print('dz/dx:', grad['x'])  # 2*x => 4\n",
    "print('dz/dy:', grad['y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nf8V52AChwhM"
   },
   "source": [
    "## 自定义梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VLJDfkXidAx"
   },
   "source": [
    "在某些情况下，您可能需要精确控制梯度的计算方式，而不是使用默认值。这些情况包括：\n",
    "\n",
    "* 你正在编写的新操作没有定义梯度。\n",
    "* 默认计算在数值上不稳定。\n",
    "* 你希望在向前传播的过程中缓存一个开销很大的计算。\n",
    "* 你打算在不修改梯度的情况下修改某个值（例如使用：tf.clip_by_value, tf.math.round)。\n",
    "\n",
    "对于编写新的操作，可以使用tf.RegisterGradient 建立。详见[该页](https://tensorflow.google.cn/api_docs/python/tf/RegisterGradient)。（请注意梯度注册表是全局的，因此请小心更改。）\n",
    "\n",
    "对于后三种情况，可以使用[tf.custom_gradient](https://tensorflow.google.cn/api_docs/python/tf/custom_gradient).\n",
    "\n",
    "下面的例子将tf.clip_by_norm应用到中间梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPwtPgxsj04C"
   },
   "outputs": [],
   "source": [
    "# 建立恒等操作，但在梯度传播的过程中裁剪\n",
    "@tf.custom_gradient\n",
    "def clip_gradients(y):\n",
    "  def backward(dy):\n",
    "    return tf.clip_by_norm(dy, 0.5)\n",
    "  return y, backward\n",
    "\n",
    "v = tf.Variable(2.0)\n",
    "with tf.GradientTape() as t:\n",
    "  output = clip_gradients(v * v)\n",
    "print(t.gradient(output, v))  # 调用backward方法，将4裁剪为2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YM9pz6O6kcxc"
   },
   "source": [
    "详见见tf.custom_gradient以查看更多细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnGSOgqtkhq9"
   },
   "source": [
    "## 多重tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plzELHImklJz"
   },
   "source": [
    "多个tape可以相互交互，例如，这里每个tape监视不同的张量集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4okpgCh7kwEB"
   },
   "outputs": [],
   "source": [
    "x0 = tf.constant(0.0)\n",
    "x1 = tf.constant(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\n",
    "  tape0.watch(x0)\n",
    "  tape1.watch(x1)\n",
    "\n",
    "  y0 = tf.math.sin(x0)\n",
    "  y1 = tf.nn.sigmoid(x1)\n",
    "\n",
    "  y = y0 + y1\n",
    "\n",
    "  ys = tf.reduce_sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mWzDZUWYkyJ6"
   },
   "outputs": [],
   "source": [
    "tape0.gradient(ys, x0).numpy()   # cos(x) => 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnqzw3nZkzsa"
   },
   "outputs": [],
   "source": [
    "tape1.gradient(ys, x1).numpy()   # sigmoid(x1)*(1-sigmoid(x1)) => 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZnksLjulOj6"
   },
   "source": [
    "## 高阶梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLKGIdMZlWPr"
   },
   "source": [
    "为了达到自动微分的目的，我们可以记录GradientTape上下文管理器中的操作。如果在上下文中计算梯度，那么梯度的计算值也会被记录下来。因此，同样的接口也适用于高阶梯度。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEiPf_95nI8x"
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(1.0)  \n",
    "\n",
    "with tf.GradientTape() as t2:\n",
    "  with tf.GradientTape() as t1:\n",
    "    y = x * x * x\n",
    "\n",
    "  # 在外部的“t2”上下文管理器中计算梯度\n",
    "  # 这意味着梯度计算也是可微的\n",
    "  dy_dx = t1.gradient(y, x)\n",
    "d2y_dx2 = t2.gradient(dy_dx, x)\n",
    "\n",
    "print('dy_dx:', dy_dx.numpy())  # 3 * x**2 => 3.0\n",
    "print('d2y_dx2:', d2y_dx2.numpy())  # 6 * x => 6.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwu4obYknEbb"
   },
   "source": [
    "虽然这确实为你提供了标量函数的二阶导数，但是由于GradientTape.gradient仅计算标量的梯度，因此该模式不能推广生成海森矩阵。要构造海森矩阵，请参见“ 雅可比”部分下的海森矩阵示例 。 \n",
    "\n",
    "在用梯度计算标量时，“嵌套调用GradientTape.gradient ”是一个很好的方法，然后所得的标量将用作第二个梯度计算的源，如以下示例所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1TB-MImndLR"
   },
   "source": [
    "## 示例：输入梯度正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ng1DsjX3nghq"
   },
   "source": [
    "许多模型容易受到“对抗性样本”的影响。这套技术修改了模型的输入，以打乱模型的输出。 最简单的方法是沿着输出相对于输入的梯度前进一步。\n",
    "\n",
    "输入梯度正则化可提高对抗性样本的鲁棒性 ，它试图使输入梯度最小化。如果输入梯度很小，那么输出的变化也应该很小。 \n",
    "\n",
    "以下是输入梯度正则化的简单步骤。方法是： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3zqFuHnn29K"
   },
   "source": [
    "1. 使用内部的tape计算输出相对于输入的梯度。 \n",
    "2. 计算该输入梯度的大小。 \n",
    "3. 计算出该梯度相对于模型的梯度。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlokU_pJopFa"
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "\n",
    "layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3Pqy4w5oqM-"
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as t2:\n",
    "  # 内部tape只考虑输入的梯度, 而非变量.\n",
    "  with tf.GradientTape(watch_accessed_variables=False) as t1:\n",
    "    t1.watch(x)\n",
    "    y = layer(x)\n",
    "    out = tf.reduce_sum(layer(x)**2)\n",
    "  # 1. 计算输入梯度.\n",
    "  g1 = t1.gradient(out, x)\n",
    "  # 2. 计算输入梯度的大小.\n",
    "  g1_mag = tf.norm(g1)\n",
    "\n",
    "# 3. 计算相对于模型的梯度\n",
    "dg1_mag = t2.gradient(g1_mag, layer.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sA7jvY8TorxC"
   },
   "outputs": [],
   "source": [
    "[var.shape for var in dg1_mag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHcSLM7mov-O"
   },
   "source": [
    "## 雅可比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cq-P6sNNozki"
   },
   "source": [
    "前面所有的例子都采用标量相对于某个源张量的梯度。\n",
    "\n",
    "雅可比矩阵表示向量值函数的梯度。每行包含一个向量分量的梯度。\n",
    "\n",
    "GradientTape.jacobian方法可帮你高效地计算雅可比矩阵。\n",
    "\n",
    "请注意：\n",
    "* 像梯度一样： sources参数可以是张量或张量容器。 \n",
    "* 与梯度不同： target张量必须是单个张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IrgoE112pejT"
   },
   "source": [
    "### 标量源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Bg5s8IOpfXq"
   },
   "source": [
    "作为第一个例子，这是矢量-目标相对于标量-源的雅可比行列式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4r1Dmh5ptnr"
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(-10.0, 10.0, 200+1)\n",
    "delta = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = tf.nn.sigmoid(x+delta)\n",
    "\n",
    "dy_dx = tape.jacobian(y, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYKEf4NkqKkz"
   },
   "source": [
    "当对标量取雅可比时，结果就是目标的形状，并给出每个元素相对于源的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fqZU4p1lqPNx"
   },
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "print(dy_dx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CDIceJAqTjj"
   },
   "outputs": [],
   "source": [
    "plt.plot(x.numpy(), y, label='y')\n",
    "plt.plot(x.numpy(), dy_dx, label='dy/dx')\n",
    "plt.legend()\n",
    "_ = plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "75ZoMZQ5qdKs"
   },
   "source": [
    "### 张量源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6z4XriLqh37"
   },
   "source": [
    "无论输入是标量还是张量， GradientTape.jacobian都能有效地计算源中每个元素相对于目标中每个元素的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PEJ60CYvql9R"
   },
   "source": [
    "例如，该层的输出的形状为(10,7)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOIxU3nlqutq"
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y = layer(x)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "goKmZ7C7q7lr"
   },
   "source": [
    "层的内核形状为(5,10)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzQUb7_zrAh7"
   },
   "outputs": [],
   "source": [
    "layer.kernel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mEkU8TPfrXqz"
   },
   "source": [
    "输出相对于内核的雅可比行列式的形状是这两个形状连接在一起后的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kE-dWBPUrejJ"
   },
   "outputs": [],
   "source": [
    "j = tape.jacobian(y, layer.kernel)\n",
    "j.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMyjPKYPrmvK"
   },
   "source": [
    "如果对目标维度进行求和，则和的梯度是可通过GradientTape.gradient计算得出的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mm1P9kpnrxJt"
   },
   "outputs": [],
   "source": [
    "g = tape.gradient(y, layer.kernel)\n",
    "print('g.shape:', g.shape)\n",
    "\n",
    "j_sum = tf.reduce_sum(j, axis=[0, 1])\n",
    "delta = tf.reduce_max(abs(g - j_sum)).numpy()\n",
    "assert delta < 1e-3\n",
    "print('delta:', delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9O0rZFMXr-ed"
   },
   "source": [
    "### 例子：海森矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lg5odX9xr_Vq"
   },
   "source": [
    "尽管tf.GradientTape没有提供用于构造海森矩阵的显式方法，但可以使用GradientTape.jacobian方法来构建矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IdtL_FrsELa"
   },
   "source": [
    "注意： 海森矩阵包含N**2参数。由于一些原因，对于大多数模型来说，这是不可能的。本例的更多内容是关于如何使用GradientTape.jacobian方法的说明，而不是对直接基于Hessian优化的解释。使用嵌套tape可有效地计算海森矩阵向量乘积，这个方法可有效的用于二阶优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yockTtkqsN6K"
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.relu)\n",
    "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.relu)\n",
    "\n",
    "with tf.GradientTape() as t2:\n",
    "  with tf.GradientTape() as t1:\n",
    "    x = layer1(x)\n",
    "    x = layer2(x)\n",
    "    loss = tf.reduce_mean(x**2)\n",
    "\n",
    "  g = t1.gradient(loss, layer1.kernel)\n",
    "\n",
    "h = t2.jacobian(g, layer1.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_y2-b59asQBj"
   },
   "outputs": [],
   "source": [
    "print(f'layer.kernel.shape: {layer1.kernel.shape}')\n",
    "print(f'h.shape: {h.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojcZprUesUrD"
   },
   "source": [
    "要将此海森矩阵用于牛顿方法，你首先需要将其轴展平为矩阵，然后将梯度展平为向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yDIlKh8saLT"
   },
   "outputs": [],
   "source": [
    "n_params = tf.reduce_prod(layer1.kernel.shape)\n",
    "g_vec = tf.reshape(g, [n_params, 1])\n",
    "h_mat = tf.reshape(h, [n_params, n_params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEeVmiJDs0nC"
   },
   "source": [
    "海森矩阵应该是对称的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzs_hNcTsz0O"
   },
   "outputs": [],
   "source": [
    "def imshow_zero_center(image, **kwargs):\n",
    "  lim = tf.reduce_max(abs(image))\n",
    "  plt.imshow(image, vmin=-lim, vmax=lim, cmap='seismic', **kwargs)\n",
    "  plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Ax3CN7fsthZ"
   },
   "outputs": [],
   "source": [
    "imshow_zero_center(h_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcsjfgPEssA8"
   },
   "source": [
    "牛顿的方法更新步骤如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ID5nZoWdtBHj"
   },
   "outputs": [],
   "source": [
    "eps = 1e-3\n",
    "eye_eps = tf.eye(h_mat.shape[0])*eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVNRBwUctGv7"
   },
   "source": [
    "注意： 不要转置矩阵 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHMLs5hRtKmU"
   },
   "outputs": [],
   "source": [
    "# X(k+1) = X(k) - (∇²f(X(k)))^-1 @ ∇f(X(k))\n",
    "# h_mat = ∇²f(X(k))\n",
    "# g_vec = ∇f(X(k))\n",
    "update = tf.linalg.solve(h_mat + eye_eps, g_vec)\n",
    "\n",
    "# 重塑更新并将其应用于变量。\n",
    "_ = layer1.kernel.assign_sub(tf.reshape(update, layer1.kernel.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSNKspAYtM08"
   },
   "source": [
    "尽管对于单个tf.Variable而言这相对简单，但将其应用于非平凡模型将需要仔细级联和切片，以在多个变量之间产生完整的海森矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8EApQMqtZRs"
   },
   "source": [
    "## 批量雅可比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GP9Vpp5tb8r"
   },
   "source": [
    "在某些情况下，你想要获取目标堆栈相对于源堆栈的雅可比行列式，其中每个目标-源堆栈对的雅可比行列式都是独立的。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xL9_71vJtnyj"
   },
   "source": [
    "例如，此处输入x的形状为(batch, ins) ，而输出y形状为(batch, outs) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibu7C-H0trx7"
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "\n",
    "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)\n",
    "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
    "  tape.watch(x)\n",
    "  y = layer1(x)\n",
    "  y = layer2(y)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCE2ug74twxU"
   },
   "source": [
    "虽然您只想要（batch，ins，outs），可y相对于x的完整雅可比矩阵的形状是（batch，ins，batch，outs）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNiiSWd6t_mT"
   },
   "outputs": [],
   "source": [
    "j = tape.jacobian(y, x)\n",
    "j.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JloJie-MuIwD"
   },
   "source": [
    "如果堆栈中每个项的梯度都是独立的，则该张量的每个(batch, batch)切片都是对角矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBr5nx-PuKCE"
   },
   "outputs": [],
   "source": [
    "imshow_zero_center(j[:, 0, :, 0])\n",
    "_ = plt.title('A (batch, batch) slice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPMLLp-xuPyq"
   },
   "outputs": [],
   "source": [
    "def plot_as_patches(j):\n",
    "  # 重新排列轴，使对角线每个都形成一个连续的面片。\n",
    "  j = tf.transpose(j, [1, 0, 3, 2])\n",
    "  # 在每个补丁之间填充。\n",
    "  lim = tf.reduce_max(abs(j))\n",
    "  j = tf.pad(j, [[0, 0], [1, 1], [0, 0], [1, 1]],\n",
    "             constant_values=-lim)\n",
    "  # R形成一个单一的形象.\n",
    "  s = j.shape\n",
    "  j = tf.reshape(j, [s[0]*s[1], s[2]*s[3]])\n",
    "  imshow_zero_center(j, extent=[-0.5, s[2]-0.5, s[0]-0.5, -0.5])\n",
    "\n",
    "plot_as_patches(j)\n",
    "_ = plt.title('All (batch, batch) slices are diagonal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mYDJsz-udDj"
   },
   "source": [
    "要获得所需的结果，可以对重复的批处理维度求和，或者使用tf.einsum选择对角线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHW-ar9Qujqs"
   },
   "outputs": [],
   "source": [
    "j_sum = tf.reduce_sum(j, axis=2)\n",
    "print(j_sum.shape)\n",
    "j_select = tf.einsum('bxby->bxy', j)\n",
    "print(j_select.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNNcsP4WuqrK"
   },
   "source": [
    "在没有额外维度的情况下进行计算会更加高效。 GradientTape.batch_jacobian方法可以做到这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGujGo9mut3E"
   },
   "outputs": [],
   "source": [
    "jb = tape.batch_jacobian(y, x)\n",
    "jb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1JnHCpFnuwpz"
   },
   "outputs": [],
   "source": [
    "error = tf.reduce_max(abs(jb - j_sum))\n",
    "assert error < 1e-3\n",
    "print(error.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILA6JEEiu7Cq"
   },
   "source": [
    "警告： GradientTape.batch_jacobian仅验证源和目标的第一个维度是否匹配。它不检查梯度是否是独立的。用户需要确保他们每次使用batch_jacobian都有意义。例如添加一个layers.BatchNormalization破坏了独立性，因为它跨批次维度进行了规范化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEEMzlJWu9mj"
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "\n",
    "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)\n",
    "bn = tf.keras.layers.BatchNormalization()\n",
    "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
    "  tape.watch(x)\n",
    "  y = layer1(x)\n",
    "  y = bn(y, training=True)\n",
    "  y = layer2(y)\n",
    "\n",
    "j = tape.jacobian(y, x)\n",
    "print(f'j.shape: {j.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0-AdKcqvCBr"
   },
   "outputs": [],
   "source": [
    "plot_as_patches(j)\n",
    "\n",
    "_ = plt.title('These slices are not diagonal')\n",
    "_ = plt.xlabel(\"Don't use `batch_jacobian`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20dQ47SzvGjS"
   },
   "source": [
    "在本例中，batch_jacobian仍然可运行并返回具有预期形状的内容，但其内容的含义不明确。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nfs7gRDjvLmh"
   },
   "outputs": [],
   "source": [
    "jb = tape.batch_jacobian(y, x)\n",
    "print(f'jb.shape: {jb.shape}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "高阶自动微分.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py37tf2",
   "language": "python",
   "name": "py37tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
