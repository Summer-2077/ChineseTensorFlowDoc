{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhNy9r6sfvEx"
   },
   "source": [
    "# 通过内置方法进行训练和评估\n",
    "\n",
    "### 引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDG14zP3XLPC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFHZwXD3g9MP"
   },
   "source": [
    "### 介绍\n",
    "\n",
    "本指南涵盖在使用内置API进行训练和验证时的训练，评估和预测(推断)模型(例如`model.fit()`，`model.evaluate()`，`model.predict()`)。\n",
    "\n",
    "如果你有兴趣在自定义的训练步骤方法中利用`fit()`，请参阅指南[“自定义fit()的内容”](https://tensorflow.google.cn/guide/keras/customizing_what_happens_in_fit/)。\n",
    "\n",
    "如果你有兴趣从头开始编写自己的训练和评估循环，请参阅指南[“从头开始编写训练循环”](https://tensorflow.google.cn/guide/keras/writing_a_training_loop_from_scratch/)。\n",
    "\n",
    "通常情况下，无论你是使用内置循环还是编写自己的循环，模型训练和评估都在每种Keras模型中严格按照相同的方式工作--Sequential模型、使用函数式API构建的模型以及从头开始编写的子类化模型。\n",
    "\n",
    "本指南不涉及分布式训练。想要了解有关分布式训练的内容，请参阅[多GPU和分布式训练](https://www.tensorflow.org/guides/distributed_training)指南。\n",
    "\n",
    "### API概述：第一个端到端示例\n",
    "\n",
    "将数据传递到模型的内置训练循环时，应该使用**NumPy数组**(如果数据较小并且适合在内存中计算)或`tf.data.Dataset`对象。在接下来的几段中，我们将[MNIST](http://yann.lecun.com/exdb/mnist/)数据集用作NumPy数组，以演示如何使用优化器，损失函数和性能指标。\n",
    "\n",
    "让我们思考以下模型(在这里，我们构建时使用函数式API，但是它也可以是Sequential模型或子类化模型)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSTBDNFUmlZ8"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sTH-k0OnIcQ"
   },
   "source": [
    "这是典型的端到端工作流程，包括：\n",
    "+ 训练\n",
    "+ 由原始训练数据中生成的验证集进行验证\n",
    "+ 在测试集上进行评估\n",
    "\n",
    "我们将会使用MNIST数据集进行示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "IpZWn2Itnqof",
    "outputId": "d27be38b-90ca-4553-fed7-afa9cda17512"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# 预处理数据(这些是NumPy数组)\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# 保留10000个样本作为验证集\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4LUNhPZog3N"
   },
   "source": [
    "接下来，我们指定训练配置(优化器，损失，指标)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PuAlNymorLH"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=keras.optimizers.RMSprop(),  # 优化器\n",
    "  # 最小化损失函数\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "  # 用于监控的性能指标列表\n",
    "  metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8dmwG8LApFni"
   },
   "source": [
    "我们调用`fit()`，它将数据切成大小为“ `batch_size`”的批量，并依据给定的“`epochs`”，重复遍历整个数据集来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "1tOL2YncpnvQ",
    "outputId": "cabd2b5d-4112-4913-830e-bd150f157572"
   },
   "outputs": [],
   "source": [
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "  x_train,\n",
    "  y_train,\n",
    "  batch_size=64,\n",
    "  epochs=2,\n",
    "  # 在每次epoch结束时，我们都会通过一些验证数据来监控验证损失和指标\n",
    "  validation_data=(x_val, y_val),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2qBtvN0OsqB"
   },
   "source": [
    "返回的“history”对象保留训练期间的损失值和指标的记录："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "KFAKPJ5KO4v4",
    "outputId": "26b0c740-6092-4d22-b132-cadd00cf6324"
   },
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLfM4dQvO8bG"
   },
   "source": [
    "我们通过`validate()`在测试数据上评估模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "y5iXYUzHPLuW",
    "outputId": "db4e1f5b-ae86-462b-90ff-75b67bb24168"
   },
   "outputs": [],
   "source": [
    "# 使用`evaluate'在测试数据上评估模型\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# 生成预测（概率-最后一层的输出）\n",
    "# 新数据使用`predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aGlCtwrP5Do"
   },
   "source": [
    "现在，让我们来复盘这个工作流程的细节\n",
    "\n",
    "### compile()方法：指定损失，指标和优化器\n",
    "\n",
    "要使用`fit()`训练模型，你需要指定损失函数，优化器以及可选的一些要监控的指标。\n",
    "\n",
    "你将它们作为compile()方法的参数传递给模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpKDrBkoQczi"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7itDQilQ1fH"
   },
   "source": [
    "`metrics`参数应为列表-你的模型可以具有任意数量的指标。\n",
    "\n",
    "如果模型具有多个输出，则可以为每个输出指定不同的损失和指标，并且可以调整每个输出对模型总损失的影响。你可以在后面“**将数据传递到多输入，多输出模型**”小节中，找到有关此内容的更多详细信息。\n",
    "\n",
    "值得一提的是，如果你经常使用默认设置，那么在许多情况下，可以通过字符串标识符，引用优化器，损失和指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elGsBzB9Skub"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=\"rmsprop\",\n",
    "  loss=\"sparse_categorical_crossentropy\",\n",
    "  metrics=[\"sparse_categorical_accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9lPxDgDS3cP"
   },
   "source": [
    "为了方便后面的重用，在这里我们模型定义和编译步骤封装为函数。我们将在本指南的不同示例中多次调用它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh5gf_pLTMje"
   },
   "outputs": [],
   "source": [
    "def get_uncompiled_model():\n",
    "  inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "  x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "  x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "  outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  return model\n",
    "\n",
    "\n",
    "def get_compiled_model():\n",
    "  model = get_uncompiled_model()\n",
    "  model.compile(\n",
    "      optimizer=\"rmsprop\",\n",
    "      loss=\"sparse_categorical_crossentropy\",\n",
    "      metrics=[\"sparse_categorical_accuracy\"],\n",
    "  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaOzcKvgTRMR"
   },
   "source": [
    "#### 许多可用的内置优化器，损失和指标\n",
    "\n",
    "通常情况下，你不必从头开始创建自己的损失，指标或优化器，因为大部分所需的可能已经封装在Keras API中：\n",
    "\n",
    "优化器：\n",
    "+ `SGD()`\n",
    "+ `RMSprop()`\n",
    "+ `Adam()`\n",
    "+ 等等\n",
    "\n",
    "损失：\n",
    "\n",
    "+ `MeanSquareError()`\n",
    "+ `KLDivergence()`\n",
    "+ `CosineSimilarity()`\n",
    "+ 等等\n",
    "\n",
    "指标：\n",
    "+ `AUC()`\n",
    "+ `Precision()`\n",
    "+ `Recall()`\n",
    "+ 等等\n",
    "\n",
    "#### 自定义损失\n",
    "使用Keras提供两种方式来支持自定义损失。第一个示例创建一个接受输入`y_true`和`y_pred`的函数。第二个示例展示了一个损失函数，该函数计算实际数据和预测之间的均方误差：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "rXSlP1kSV1IU",
    "outputId": "341cb2c2-1b3b-4112-e8b1-227ccef95a73"
   },
   "outputs": [],
   "source": [
    "def custom_mean_squared_error(y_true, y_pred):\n",
    "  return tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)\n",
    "\n",
    "# 我们需要对标签进行ont-hot编码才能使用MSE\n",
    "y_train = tf.cast(y_train, tf.int32)\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DBhP7VftWqn8"
   },
   "source": [
    "如果你自定义的损失函数，需要使用除`y_true`和`y_pred`的其他参数，则可以对[`tf.keras.losses.Loss`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss)类进行子类化，并实现以下两种方法：\n",
    "\n",
    "+ `__init __(self)`：接受在损失函数调用期间传递的参数\n",
    "\n",
    "+ `call(self，y_true，y_pred)`：使用目标（y_true）和模型预测（y_pred）计算模型的损失\n",
    "\n",
    "假设你要使用均方误差，需要加上一个附加项，使得预测值远远偏离0.5（我们假设分类目标是one_hot编码的，且取值介于0到1之间）。这激励了模型不要过于自信，这可能有助于减少过度拟合（在尝试之前，我们不知道它是否有效！）。\n",
    "\n",
    "下面示例如何实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "YbpPfXmoY2_8",
    "outputId": "b653ece5-5013-4968-d71e-a22f5b80d24c"
   },
   "outputs": [],
   "source": [
    "class CustomMSE(keras.losses.Loss):\n",
    "  def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
    "    super().__init__(name=name)\n",
    "    self.regularization_factor = regularization_factor\n",
    "\n",
    "  def call(self, y_true, y_pred):\n",
    "    mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "    reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
    "    return mse + reg * self.regularization_factor\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n",
    "\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bBv-nIHHb3SN"
   },
   "source": [
    "#### 自定义指标\n",
    "\n",
    "如果需要的指标不在API中，则可以通过继承`tf.keras.metrics.Metric`类的方式，轻松创建自定义指标。你将需要实现如下4种方法：\n",
    "+ `__init__(self)`，在其中你将为指标创建状态变量。\n",
    "+ `update_state(self, y_true, y_pred, sample_weight=None)`，它使用目标`y_true`和模型预测`y_pred`更新状态变量。\n",
    "+ `result(self)`，它使用状态变量来计算最终结果。\n",
    "+ `reset_states(self)`，重新初始化指标的状态。\n",
    "\n",
    "状态更新和结果计算需要分开(分别在`update_state()`和`result()`中)，因为在某些情况下，计算结果的代价可能非常高，所以只能定期执行。\n",
    "\n",
    "下面是一个简单的示例，展示了如何实现`CategoricalTruePositives`指标，该指标计算多少个样本被正确分类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "yWHIbMPXdapF",
    "outputId": "ad4cc4a8-8482-4140-8d7e-91890d915f92"
   },
   "outputs": [],
   "source": [
    "class CategoricalTruePositives(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
    "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
    "        values = tf.cast(values, \"float32\")\n",
    "        if sample_weight is not None:  # 样本加权\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "\n",
    "    def reset_states(self):\n",
    "        # 每轮开始，指标重置\n",
    "        self.true_positives.assign(0.0)\n",
    "\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[CategoricalTruePositives()],\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcQm234aiBgY"
   },
   "source": [
    "#### 非标准的方式处理损失和指标\n",
    "\n",
    "可以利用`y_true`和`y_pred`计算出绝大多数损失和指标，其中`y_pred`是模型的输出，但并不意味着所有损失和指标都能计算。例如，正则化损失可能仅需要层的激活(在这种情况下没有目标)，并且此激活可能不是模型输出。\n",
    "\n",
    "在这种情况下，你可以在自定义层的调用方法内部调用`self.add_loss(loss_value)`。以这种方式添加的损失会在训练期间添加到“主要”损失中(传递给`compile()`的损失)。\n",
    "\n",
    "下面是一个添加正则化的简单示例(请注意，活动正则化内置在所有Keras层中--下面的层仅是为了提供一个具体示例)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "QMyOymtqjsYS",
    "outputId": "70313874-6653-44b1-a978-174a13709752"
   },
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
    "        return inputs  # 输入不做任何处理\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# 将活动正则化插入为一层\n",
    "x = ActivityRegularizationLayer()(x)  # 对本层输入或前一层的输出添加损失\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "# 由于正则化组件，显示的损失将比之前的损失高得多。\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nb3xkM19kmTo"
   },
   "source": [
    "你可以使用`add_metric()`对记录的指标值执行相同的操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "CSgTkOjYcaXR",
    "outputId": "4b95ab82-2115-43e0-c200-53002441763b"
   },
   "outputs": [],
   "source": [
    "class MetricLoggingLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # `aggregation`参数定义了在每个epoch中如何汇总每个\n",
    "        # 批处理值：在下面的情况下，我们只需对它们进行平均即可。\n",
    "        self.add_metric(\n",
    "            keras.backend.std(inputs), name=\"std_of_activation\", aggregation=\"mean\"\n",
    "        )\n",
    "        return inputs  # 输入不做任何处理\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# 将其插入为一层\n",
    "x = MetricLoggingLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bESyG-S7l1oq"
   },
   "source": [
    "在函数式API中，你可以调用`model.add_loss(loss_tensor)`或者`model.add_metric(metric_tensor, name, aggregation)`\n",
    "\n",
    "下面简单举例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "QiUIPisrmAWa",
    "outputId": "a3cf7753-e126-4e51-ec72-d5889c66cf24"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
    "\n",
    "model.add_metric(keras.backend.std(x1), name=\"std_of_activation\", aggregation=\"mean\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVQb3xrfmtEm"
   },
   "source": [
    "注意了，当你通过`add_loss()`传递损失时，可以在没有损失函数的情况下调用`compile()`，因为该模型已经具有最小化的损失。\n",
    "\n",
    "思考以下`LogisticEndpoint`层：它以targets和logits作为输入，并通过`add_loss()`跟踪交叉熵损失。它还通过`add_metric()`跟踪分类准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DHwbkB7np_H"
   },
   "outputs": [],
   "source": [
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "  def __init__(self, name=None):\n",
    "      super(LogisticEndpoint, self).__init__(name=name)\n",
    "      self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "      self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "  def call(self, targets, logits, sample_weights=None):\n",
    "      # 计算训练时间损失值并用`self.add_loss()`添加到层\n",
    "      loss = self.loss_fn(targets, logits, sample_weights)\n",
    "      self.add_loss(loss)\n",
    "\n",
    "      # 记录准确性作为指标并用`self.add_metric()`添加到层\n",
    "      acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "      self.add_metric(acc, name=\"accuracy\")\n",
    "\n",
    "      # 返回预测张量 (通过`.predict()`).\n",
    "      return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxqQISu2oiu4"
   },
   "source": [
    "你可以在具有两个输入(输入数据和目标)的模型中使用它，而无需损失参数即可进行编译，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "NBOBMP9Nouaj",
    "outputId": "b558fb82-72a0-47a6-e469-5d8171e9ecce"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")  # 没有损失函数参数\n",
    "\n",
    "data = {\n",
    "    \"inputs\": np.random.random((3, 3)),\n",
    "    \"targets\": np.random.random((3, 10)),\n",
    "}\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pD42Kja-pM-j"
   },
   "source": [
    "有关训练多输入模型的更多信息，请参阅**将数据传递到多输入，多输出模型**小节。\n",
    "\n",
    "#### 自动划分验证集\n",
    "\n",
    "在你看到的第一个端到端示例中，我们使用了`validation_data`参数将NumPy数组`(x_val，y_val)`的元组传递给模型，用来在每次epoch结束时评估验证损失和验证指标。\n",
    "\n",
    "下面演示另一种方式：可以利用参数`validate_split`，自动保留部分训练数据以供验证使用。参数值表示要保留用于验证的数据量大小，因此应将其设置为大于0且小于1的数字。例如，`validation_split = 0.2`表示“使用20％的数据进行验证”，而`validation_split = 0.6`表示“使用60％的数据进行验证”。\n",
    "\n",
    "验证的计算方式是在进行打乱之前，通过调用fit接收数据集的最后x％采样。\n",
    "\n",
    "请注意，仅在使用NumPy数据进行训练时才能使用`validation_split`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "PGf1jxTIq5tP",
    "outputId": "bf6a895d-b20c-42f1-aa05-dc33f9a38c90"
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UON590Rq9hS"
   },
   "source": [
    "### 使用tf.data.Datasets进行训练和评估\n",
    "\n",
    "在前面的介绍中，你已经了解了如何处理损失，指标和优化器，并且已经了解了当传递的数据为NumPy数组时，如何使用`validation_data`和`validation_split`参数进行训练。\n",
    "\n",
    "现在让我们看一下数据以`tf.data.Dataset`对象形式出现的情况。\n",
    "\n",
    "`tf.data`API是TensorFlow 2.0中的一组非常实用的工具，用于以快速且可扩展的方式加载和预处理数据。\n",
    "\n",
    "有关创建`Datasets`的完整指南，请参阅[tf.data文档](https://tensorflow.google.cn/guide/data)。\n",
    "\n",
    "你可以将`Dataset`实例直接传递给`fit()`，`evaluate()`和`predict()`方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "pYlePVOdsfuA",
    "outputId": "6a33aecf-251b-49e1-d8d5-9ecd5e7af5a7"
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# 首先创建Dataset实例\n",
    "# 为了方便，我们将使用与之前的MNIST数据\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# 打乱和切片数据集\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# 现在我们处理测试集\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "# 因为我们的数据集已经是批量的\n",
    "# 所以我们不需要传递`batch_size`参数\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# 接下来你就可以评估或预测数据集了\n",
    "print(\"Evaluate\")\n",
    "result = model.evaluate(test_dataset)\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtSXDo_AtU3A"
   },
   "source": [
    "请注意，数据集会在每次epoch结束时重置，因此可以在下一次epoch中重复使用。\n",
    "\n",
    "如果你只想对来自此`Dataset`的特定批次进行训练，则可以传递`steps_per_epoch`参数，该参数指定在执行下一次epoch之前，应该使用多少批量的数据。\n",
    "\n",
    "如果执行此操作，则不会在每次epoch结束时重置数据集，而是继续执行下一批。数据集最终将用完数据(除非它是无限循环的数据集)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "G1P42uR3uxC0",
    "outputId": "0458c2fe-c1d3-4b9c-a606-4273009a6513"
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# 准备训练数据\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# 每次epoch只使用100批量 (这里的数据是 100 * 64 样本)\n",
    "model.fit(train_dataset, epochs=3, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNiJ_paewthy"
   },
   "source": [
    "#### 使用验证数据集\n",
    "\n",
    "你可以在`fit()`中，将`Dataset`实例作为`validation_data`参数进行传递："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "BHB_iyIuw8q0",
    "outputId": "3f637a6e-a59f-4011-f615-6b6ae62ac77f"
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# 准备训练数据\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# 准备验证数据\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=1, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7S15NzRbxF6s"
   },
   "source": [
    "在每次epoch结束时，模型将遍历验证数据集并计算验证损失和验证指标。\n",
    "\n",
    "如果只想对该数据集中的特定批次进行验证，则可以传递`validation_steps`参数，该参数指定在中断验证并进入下一个epoch之前，模型应该使用多少批量的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "5wiMbgfCx7tL",
    "outputId": "1dc6f0c3-49c5-460b-a208-873f2fc54c72"
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# 准备训练数据\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# 准备验证数据\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1,\n",
    "    # 通过`validation_steps`参数，指定仅使用数据集的前10批进行验证\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W7xB-bbUyTEm"
   },
   "source": [
    "请注意，验证数据集将在每次使用后重置（因此你将始终在每次epoch过程中，评估重复的样本）。\n",
    "\n",
    "当使用`Dataset`对象进行训练时，不支持参数`validation_split`（从训练数据生成保留集），因为此功能需要能够索引样本数据集，而这通常是`Dataset`API不可能实现的。\n",
    "\n",
    "### 支持其他输入格式\n",
    "除了NumPy数组，张量和TensorFlow的`Dataset`外，还可以使用Pandas数据框架或从生成大量数据和标签的Python生成器中训练Keras模型。\n",
    "\n",
    "特别是，[`keras.utils.Sequence`](https://tensorflow.google.cn/api_docs/python/tf/keras/utils/Sequence)类提供了一个简单的接口来构建Python数据生成器，该数据生成器可以完成多批次导入处理并且可以打乱。\n",
    "\n",
    "通常情况下，我们建议你按照如下准则使用：\n",
    "+ NumPy输入数据（如果你的数据很小并且适合在内存中计算）\n",
    "+ 如果你有大型数据集，并且需要进行分布式训练，请使用`Dataset`对象\n",
    "+ 如果你有大型数据集，并且需要执行很多自定义的Python端处理，而这些处理在TensorFlow中无法完成（例如，如果你依赖外部库进行数据加载或预处理），请使用`Sequence`对象\n",
    "\n",
    "### 使用keras.utils.Sequence作为输入\n",
    "\n",
    "`keras.utils.Sequence`是一个非常实用的工具，你可以将其子类化以获取具有两个重要属性的Python生成器：\n",
    "+ 它适用于多处理。\n",
    "+ 它可以打乱(比如，当为`fit()`传递`shuffle=True`时)\n",
    "\n",
    "继承`Sequence`必须实现以下两个方法：\n",
    "+ `__getitem__`\n",
    "+ `__len__`\n",
    "\n",
    "`__getitem__`方法应返回整个批量。如果要在各个epoch之间修改数据集，则可以实现`on_epoch_end`方法。\n",
    "\n",
    "下面是一个简单的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfCueRo12ZN1"
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "# `filenames`是一个图片路径的列表\n",
    "# `labels`是对应的标签\n",
    "\n",
    "class CIFAR10Sequence(Sequence):\n",
    "    def __init__(self, filenames, labels, batch_size):\n",
    "        self.filenames, self.labels = filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):  # 多少批\n",
    "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):  # 获取一批\n",
    "        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return np.array([\n",
    "            resize(imread(filename), (200, 200))\n",
    "               for filename in batch_x]), np.array(batch_y)\n",
    "\n",
    "sequence = CIFAR10Sequence(filenames, labels, batch_size)\n",
    "model.fit(sequence, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkA8WaF83kt7"
   },
   "source": [
    "### 使用样本权重和类别权重\n",
    "\n",
    "在默认设置下，样本的权重由其在数据集中的频率决定。有两种方法可以控制数据权重，而与采样频率无关：\n",
    "\n",
    "+ 样本权重\n",
    "+ 类别权重\n",
    "\n",
    "#### 类别权重\n",
    "\n",
    "这是通过将字典传递给`Model.fit()`的`class_weigh`t参数来设置的。该词典将类别索引映射到该类别的样本权重。\n",
    "\n",
    "这可用于平衡类别而无须重新采样，或用于训练倾向于特定类别的模型。\n",
    "\n",
    "例如，如果类别“0”是数据中类别“1”的一半，则可以使用`Model.fit(...，class_weight = {0：1.，1：0.5})`。\n",
    "\n",
    "下面是一个NumPy示例，其中我们使用类别权重或样本权重来完成倾向于类别5（在MNIST数据集中的数字“5”）的正确分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "_CC8KXzo58IN",
    "outputId": "daa039f5-fb45-40a1-97e8-b09a36c5bb0e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_weight = {\n",
    "    0: 1.0,\n",
    "    1: 1.0,\n",
    "    2: 1.0,\n",
    "    3: 1.0,\n",
    "    4: 1.0,\n",
    "    # 类别5的权为2，使得它2倍重要\n",
    "    5: 2.0,\n",
    "    6: 1.0,\n",
    "    7: 1.0,\n",
    "    8: 1.0,\n",
    "    9: 1.0,\n",
    "}\n",
    "\n",
    "print(\"Fit with class weight\")\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0_KcrnN6B1t"
   },
   "source": [
    "#### 样本权重\n",
    "\n",
    "为了进行细粒度控制，或者如果你不构建分类器，则可以使用“样本权重”。\n",
    "+ 当使用NumPy数据进行训练时：将`sample_weight`参数传递给`Model.fit()`。\n",
    "+ 当使用[`tf.data`](https://tensorflow.google.cn/api_docs/python/tf/data)或任何其他迭代器进行训练时：使用`(input_batch，label_batch，sample_weight_batch)`元组。\n",
    "\n",
    "“样本权重”是一个数组，用于指定在批次中，每个样品在计算总损失时应具有的权重。它通常用于不平衡的分类问题（其想法是将更多的权重分配给鲜为人知的类）。\n",
    "\n",
    "当所使用的权重为1和0时，该数组可用作损失函数的掩码（完全丢弃某些样本对总损失的影响）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "dPdcqfPU7BoR",
    "outputId": "53536f87-48d0-4390-acc0-79a1c9c46496"
   },
   "outputs": [],
   "source": [
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "\n",
    "print(\"Fit with sample weight\")\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AO-BwTKw7Ncg"
   },
   "source": [
    "下面是使用`Dataset`作为示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "0OKwMkaN7R6n",
    "outputId": "65fac408-e649-4d1b-9853-feedf5ea29e8"
   },
   "outputs": [],
   "source": [
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "\n",
    "# 创建一个包含样本权重的Dataset\n",
    "# (返回元组中的第三个元素).\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, sample_weight))\n",
    "\n",
    "# 对数据集进行打乱和切片.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtWyXrbM7nv_"
   },
   "source": [
    "### 将数据传递到多输入，多输出模型\n",
    "\n",
    "在前面的示例中，我们考虑的模型只具有单个输入(形状的张量(764，))和单个输出(形状的预测张量(10，))。那具有多个输入或输出的模型该如何处理呢？\n",
    "\n",
    "思考以下模型，该模型具有形状为(32,32,3)(即(高度，宽度，通道))的图像输入，以及形状为(None，10)(即(时间步长，特征))的时间序列输入。我们的模型将根据这些输入的组合计算出的两个输出：“得分”(形状(1，))和五个类别(形状(5，))的概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWCqmICL8Z8J"
   },
   "outputs": [],
   "source": [
    "image_input = keras.Input(shape=(32, 32, 3), name=\"img_input\")\n",
    "timeseries_input = keras.Input(shape=(None, 10), name=\"ts_input\")\n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(image_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = layers.Dense(1, name=\"score_output\")(x)\n",
    "class_output = layers.Dense(5, activation=\"softmax\", name=\"class_output\")(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[image_input, timeseries_input], outputs=[score_output, class_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_K3XQXwL89fl"
   },
   "source": [
    "让我们绘制这个模型，以便你可以清楚地看到我们在这里做什么（请注意，图中显示的形状是批量的形状，而不是每个样本的形状）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "F5igwxbT9Imd",
    "outputId": "e730dbae-5005-48d3-9fa3-8699da5c5f2d"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rG5Ef4vD9K_Y"
   },
   "source": [
    "在编译时，通过将损失函数作为列表传递，我们可以为不同的输出指定不同的损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5vGeVWa9muS"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XQNCJhH9qCI"
   },
   "source": [
    "如果我们仅将单个损失函数传递给模型，则每个输出将应用相同的损失函数（在示例中不合适）。\n",
    "\n",
    "同样，指标也可以有相关的操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBwdLRo5-ABx"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
    "    metrics=[\n",
    "        [\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError(),\n",
    "        ],\n",
    "        [keras.metrics.CategoricalAccuracy()],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOO0yDIG-Dlb"
   },
   "source": [
    "由于我们对每个输出层进行了命名，因此我们还可以通过字典指定每个输出的损失和指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5M6o5E_e-R5N"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"score_output\": keras.losses.MeanSquaredError(),\n",
    "        \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
    "    },\n",
    "    metrics={\n",
    "        \"score_output\": [\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError(),\n",
    "        ],\n",
    "        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5p4Mhugd-Uw0"
   },
   "source": [
    "如果你有两个以上的输出，我们建议使用显式名称和字典。\n",
    "\n",
    "可以使用`loss_weights`参数为不同的于输出的损失赋予不同的权重（例如，在我们的示例中，我们可能希望通过赋予类别损失2倍的权重，来提升“得分”损失的影响）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0WnohRTL-9JV"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"score_output\": keras.losses.MeanSquaredError(),\n",
    "        \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
    "    },\n",
    "    metrics={\n",
    "        \"score_output\": [\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError(),\n",
    "        ],\n",
    "        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
    "    },\n",
    "    loss_weights={\"score_output\": 2.0, \"class_output\": 1.0},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hbl7Z_4y_BKN"
   },
   "source": [
    "如果这些输出仅用于预测而不是训练，你还可以选择不为某些输出计算损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tUATG60g_Keb"
   },
   "outputs": [],
   "source": [
    "# 使用损失列表\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[None, keras.losses.CategoricalCrossentropy()],\n",
    ")\n",
    "\n",
    "# 或者损失字典形式\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\"class_output\": keras.losses.CategoricalCrossentropy()},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IYDYvISF_R1V"
   },
   "source": [
    "将数据传递给多输入或多输出模型的方式与在编译中指定损失函数的方式类似：你可以传递**NumPy数组的列表**（以1:1映射到接收损失函数的输出）或**将输出名称映射到NumPy数组**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "X8YR144X_v7p",
    "outputId": "5bdf5642-ff37-4615-89de-264a31dd5786"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
    ")\n",
    "\n",
    "# 生成模拟NumPy数据\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# 使用列表传递数据\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=1)\n",
    "\n",
    "# 或者，使用字典\n",
    "model.fit(\n",
    "    {\"img_input\": img_data, \"ts_input\": ts_data},\n",
    "    {\"score_output\": score_targets, \"class_output\": class_targets},\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4fowN53AJPp"
   },
   "source": [
    "下面是使用`Dataset`的示例：与我们对NumPy数组所做的类似，`Dataset`应返回一个字典元组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "qKbuM0X3AVHc",
    "outputId": "4743637a-f332-45d4-a396-372826a9a5e5"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"img_input\": img_data, \"ts_input\": ts_data},  # 输入字典\n",
    "        {\"score_output\": score_targets, \"class_output\": class_targets},  # 输出字典\n",
    "    )\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJqUk5I5Ac9t"
   },
   "source": [
    "### 使用回调\n",
    "\n",
    "Keras中的回调是在训练期间的不同时间点被调用的（在epoch开始时，在批处理结束时，在epoch结束时等），因此，回调可以实现如下的行为：\n",
    "\n",
    "+ 在训练过程中的不同时间点进行验证（优于内置的按epoch验证）\n",
    "+ 定期或在超过一定精度阈值时对模型进行检查\n",
    "+ 当训练似乎停滞不前时，更改模型的学习率\n",
    "+ 当训练似乎停滞不前时，对顶层进行微调\n",
    "+ 在训练结束或超出某个性能阈值时发送电子邮件或即时消息通知\n",
    "+ 等等\n",
    "\n",
    "回调可以以列表的方式，传递给`fit()`进行调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "colab_type": "code",
    "id": "-AFnuflUoGzC",
    "outputId": "7e5da2ad-3df0-484e-855c-7081f55930d4"
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # 当`val_loss`不再提升时，停止训练\n",
    "        monitor=\"val_loss\",\n",
    "        # 将\"损失下降不超过1e-2\"作为不再提升的标准\n",
    "        min_delta=1e-2,\n",
    "        # 将\"至少执行两个epoch\"作为不再提升的标准\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVaBuHJ6qTqY"
   },
   "source": [
    "#### 有许多内置的回调可以使用\n",
    "+ `ModelCheckpoint`：定期保存模型\n",
    "+ `EarlyStopping`：当训练不再改善验证指标时，停止训练\n",
    "+ `TensorBoard`：定期在[TensorBoard](https://tensorflow.google.cn/tensorboard)中编写可视化的模型日志（更多详细信息，请参见“可视化”部分）\n",
    "+ `CSVLogger`：将损失和指标数据保存到CSV文件\n",
    "+ 等等\n",
    "\n",
    "有关完整的内置回调方法列表，请参见[回调文档](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks/)。\n",
    "\n",
    "#### 自定义回调\n",
    "\n",
    "你可以通过继承基类[`keras.callbacks.Callback`](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks/Callback)来创建自定义回调。回调可以通过类属性`self.model`访问其关联的模型。\n",
    "\n",
    "确保已经阅读了完整的[编写自定义回调的指南](https://tensorflow.google.cn/guide/keras/custom_callback/)。\n",
    "\n",
    "下面是一个简单的示例，在训练过程中保存了每个批量的损失值列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ivza2BeZsPPD"
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.per_batch_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.per_batch_losses.append(logs.get(\"loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_RXCao_sip3"
   },
   "source": [
    "#### 模型的检查点\n",
    "\n",
    "在相对较大的数据集上训练模型时，至关重要的是要定期保存模型的检查点。\n",
    "\n",
    "最简单的方法是使用`ModelCheckpoint`回调："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "-nEosYZ_s20a",
    "outputId": "a193a62b-c976-4ad8-f9fe-cfb47952294f"
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        # 保存模型的路径\n",
    "        # 下面的两个参数表示为，当且仅当`val_loss`分数提高时，才会覆盖当前检查点。\n",
    "        # 保存的模型名称将包含当前epoch。\n",
    "        filepath=\"mymodel_{epoch}\",\n",
    "        save_best_only=True,  # 仅在`val_loss`得到提升的情况下保存模型。\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    x_train, y_train, epochs=2, batch_size=64, callbacks=callbacks, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEJ4OWMGtXsr"
   },
   "source": [
    "`ModelCheckpoint`回调可用于实现容错：在随机中断训练的情况下，能够从模型的最后保存状态重新开始训练。下面是一个基本示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "SFCCDwm4ty-j",
    "outputId": "ba3c3eba-3c67-4acd-b8f2-0a7ebc75c5ff"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 准备一个目录来存储所有的检查点。\n",
    "checkpoint_dir = \"./ckpt\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "def make_or_restore_model():\n",
    "    # 如果没有检查点可用，将还原最新模型，或创建新模型\n",
    "    checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "        print(\"Restoring from\", latest_checkpoint)\n",
    "        return keras.models.load_model(latest_checkpoint)\n",
    "    print(\"Creating a new model\")\n",
    "    return get_compiled_model()\n",
    "\n",
    "\n",
    "model = make_or_restore_model()\n",
    "callbacks = [\n",
    "    # 回调中每100个批量保存一次SavedModel。\n",
    "    # 我们将训练损失包含在保存的模型名称中。\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_dir + \"/ckpt-loss={loss:.2f}\", save_freq=100\n",
    "    )\n",
    "]\n",
    "model.fit(x_train, y_train, epochs=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XTugtXlUuyHG"
   },
   "source": [
    "你也可以调用自己的回调函数来保存和恢复模型。\n",
    "\n",
    "有关序列化和保存的完整指南，请参见[模型的保存和序列化指南](https://tensorflow.google.cn/guide/keras/save_and_serialize/)。\n",
    "\n",
    "### 使用学习率策略\n",
    "\n",
    "训练深度学习模型的常见模式是随着训练的进行逐渐减少学习率。这通常称为“学习率衰减”。\n",
    "\n",
    "学习率衰减策略可以是静态的（根据当前epoch或当前批量索引预先确定），也可以是动态的（响应于模型的当前行为，尤其是验证损失）。\n",
    "\n",
    "#### 将策略传递给优化器\n",
    "\n",
    "通过将策略对象作为优化器中的`learning_rate`参数进行传递，你可以轻松使用静态学习率衰减策略："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDDSHjMEwRU7"
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oo8QocDqwZ-B"
   },
   "source": [
    "有几个内置的策略：`ExponentialDecay`，`PiecewiseConstantDeca`y，`PolynomialDecay`和`InverseTimeDecay`。\n",
    "\n",
    "#### 使用回调实现动态学习率策略\n",
    "\n",
    "由于优化器无法访问验证指标，因此无法使用这些策略对象实现动态学习率策略（例如，当验证损失不再改善时降低学习率）。\n",
    "\n",
    "然而，回调可以访问所有指标，包括验证指标！因此，你可以通过使用回调来修改优化器上的当前学习率，从而实现此模式。实际上，它已经实现为内置回调，即`ReduceLROnPlateau`。\n",
    "\n",
    "### 可视化训练期间的损失和指标\n",
    "\n",
    "在训练期间密切监控模型的最佳方法是使用**TensorBoard**，这是一个基于浏览器的应用程序，你可以在本地运行该程序，它为你提供如下功能：\n",
    "+ 损失以及用于训练和评估的指标的图表\n",
    "+ （可选）可视化图层激活的直方图\n",
    "+ （可选）你的`Embedding`层学习到的嵌入空间的3D可视化\n",
    "\n",
    "如果你是通过pip安装了TensorFlow，则应该能够从命令行启动TensorBoard："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yevAi8OByE5w"
   },
   "outputs": [],
   "source": [
    "tensorboard --logdir=/full_path_to_your_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSrof1F5ySTy"
   },
   "source": [
    "#### 使用TensorBoard回调\n",
    "\n",
    "使用`TensorBoard`回调最简单的方法是，将TensorBoard搭配Keras模型和fit方法一起使用。\n",
    "\n",
    "在最简单的情况下，只需指定你希望回调时，日志的储存位置，就可以了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "l_Cn350yzDE-",
    "outputId": "a354517a-d2ca-4e04-fa21-adfe52614a06"
   },
   "outputs": [],
   "source": [
    "keras.callbacks.TensorBoard(\n",
    "    log_dir=\"/full_path_to_your_logs\",\n",
    "    histogram_freq=0,  # 记录直方图可视化的频率\n",
    "    embeddings_freq=0,  # 记录嵌入可视化的频率\n",
    "    update_freq=\"epoch\",\n",
    ")  # 写入日志的频率（默认值：每个epoch一次）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3hXG8BYzSkz"
   },
   "source": [
    "获取更多有关信息，请参见[TensorBoard回调的文档](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/tensorboard/)。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN0TuaoFGho0sk/sm83YwX9",
   "collapsed_sections": [],
   "name": "训练和评估.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
