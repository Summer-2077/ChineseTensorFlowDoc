{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1L3zJP6pPGD"
   },
   "source": [
    "# 对抗FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8dn1-g8BpPDx"
   },
   "source": [
    "本节使用快速梯度符号方法Fast Gradient Signed Method (FGSM)攻击 生成 *对抗样本adversarial example* ，参考文献[Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572). 这是第一个也是最流行的欺骗神经网络的攻击。 \n",
    "\n",
    "## 什么是对抗样本?\n",
    "\n",
    "对抗样本是故意生成欺骗神经网络的输入，它使得网络错误分类。这些对抗输入对于人眼是不可分别的，但是会使得网络错误识别图像内容。有多种这种类型的攻击，但是，这里关注快速梯度符号方法攻击，它是一种*白盒*攻击方法，目标是产生错误分类。白盒攻击是对攻击模型可以完全访问。下面是一个来自论文的一个最有名的对抗样本。\n",
    "\n",
    "![Adversarial Example](adversarial_example.png)\n",
    "\n",
    "这里，从熊猫图像开始，攻击像原始图像加入小的扰动，结果导致图像被分类为长臂猿，而且置信度还比较高。加入扰动的过程下面解释。\n",
    "\n",
    "## 快速梯度符号法Fast gradient sign method\n",
    "\n",
    "FGSM使用神经网络的梯度产生一个对抗样本。对于输入图像，方法使用输入图像损失的梯度产生一个新图像，新图像使得损失最大。这个新图像称为对抗图像。如下公式所示：\n",
    "\n",
    "$$adv\\_x = x + \\epsilon*\\text{sign}(\\nabla_xJ(\\theta, x, y))$$\n",
    "\n",
    "其中 \n",
    "\n",
    "*   adv_x : 对抗图像.\n",
    "*   x : 原始输入图像.\n",
    "*   y : 原始图像标签.\n",
    "*   $\\epsilon$ : 确保扰动足够小的系数.\n",
    "*   $\\theta$ : 模型参数.\n",
    "*   $J$ : 损失.\n",
    "\n",
    "一个有趣的性质是梯度是对输入图像的，这是因为目标是创建一个最大化损失的图像。完成这个目标的一个方法是，确定图像中每个点对损失值的贡献，然后相应增加扰动。这个方法运行相当快，因为通过使用链式法则和寻找需要的梯度可以容易的确定每个输入像素对损失的贡献。因此，梯度是相对于图像的。而且，模型不再训练（因而梯度不是相对于可训练变量，即模型参数），模型参数保持不变。唯一的目标是欺骗已经训练好的模型。\n",
    "\n",
    "因此，我们试着愚弄一个预训练的模型，在本节，模型是 [MobileNetV2](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications/MobileNetV2) , 它是在[ImageNet](http://www.image-net.org/)上训练好的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vag2WYR6yTOC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 8)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiTHY8dqxzx7"
   },
   "source": [
    "让我们加载预训练的 MobileNetV2 模型和ImageNet类标签."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqhk2vYx6Ag0"
   },
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,\n",
    "                                                     weights='imagenet')\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "# ImageNet标签\n",
    "decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2cLrJH0zpfC"
   },
   "outputs": [],
   "source": [
    "# 图像预处理函数，使得图像处理后可以喂给MobileNetV2\n",
    "def preprocess(image):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = tf.image.resize(image, (224, 224))\n",
    "  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "  image = image[None, ...]\n",
    "  return image\n",
    "\n",
    "# 从概念向量中提取标签的函数\n",
    "def get_imagenet_label(probs):\n",
    "  return decode_predictions(probs, top=1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEZaMVFgSUA-"
   },
   "source": [
    "## 原始图像\n",
    "\n",
    "让我们使用一张[Labrador Retriever](https://commons.wikimedia.org/wiki/File:YellowLabradorLooking_new.jpg) 图像， 并从它生成对抗样本. 第一步是对它预处理，使得它可以作为MobileNetV2模型的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpYrQ4OQSYWk"
   },
   "outputs": [],
   "source": [
    "image_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
    "image_raw = tf.io.read_file(image_path)\n",
    "image = tf.image.decode_image(image_raw)\n",
    "\n",
    "image = preprocess(image)\n",
    "image_probs = pretrained_model.predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mvPlta_uSbuI"
   },
   "source": [
    "让我们看看这种图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99Jc-SNoSZot"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(image[0]*0.5+0.5) # To change [-1, 1] to [0,1]\n",
    "_, image_class, class_confidence = get_imagenet_label(image_probs)\n",
    "plt.title('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kElVTbF690CF"
   },
   "source": [
    "## 创建对抗样本\n",
    "\n",
    "### 使用FGSM\n",
    "第一步，创建用来对原始图像修改的扰动。如前所说，梯度是相对于图像的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhZxlOnuBCVr"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def create_adversarial_pattern(input_image, input_label):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(input_image)\n",
    "    prediction = pretrained_model(input_image)\n",
    "    loss = loss_object(input_label, prediction)\n",
    "\n",
    "  # 获得损失相对于输入图像的梯度\n",
    "  gradient = tape.gradient(loss, input_image)\n",
    "  # 获得创建扰动的梯度的符号\n",
    "  signed_grad = tf.sign(gradient)\n",
    "  return signed_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RbuftX0eSlDQ"
   },
   "source": [
    "产生的扰动可以可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rVjnb6M7Smv4"
   },
   "outputs": [],
   "source": [
    "# 获得图像的输入标签\n",
    "labrador_retriever_index = 208\n",
    "label = tf.one_hot(labrador_retriever_index, image_probs.shape[-1])\n",
    "label = tf.reshape(label, (1, image_probs.shape[-1]))\n",
    "\n",
    "perturbations = create_adversarial_pattern(image, label)\n",
    "plt.imshow(perturbations[0]*0.5+0.5); # To change [-1, 1] to [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKKSFHjwCyQH"
   },
   "source": [
    "可以尝试不同的系数，并观察结果图像。你将发现，随着系数epsilon的增加，欺骗网络变得容易。但是，这需要在欺骗网络和图像容易辨认间取得平衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dBtG0Kl5SspV"
   },
   "outputs": [],
   "source": [
    "def display_images(image, description):\n",
    "  _, label, confidence = get_imagenet_label(pretrained_model.predict(image))\n",
    "  plt.figure()\n",
    "  plt.imshow(image[0]*0.5+0.5)\n",
    "  plt.title('{} \\n {} : {:.2f}% Confidence'.format(description,\n",
    "                                                   label, confidence*100))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DA8g-Zp69J4"
   },
   "outputs": [],
   "source": [
    "epsilons = [0, 0.01, 0.1, 0.15]\n",
    "descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')\n",
    "                for eps in epsilons]\n",
    "\n",
    "for i, eps in enumerate(epsilons):\n",
    "  adv_x = image + eps*perturbations\n",
    "  adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
    "  display_images(adv_x, descriptions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxt5VfnXHQT6"
   },
   "source": [
    "## 下一步\n",
    "\n",
    "现在你已经找到对抗攻击，在不同的数据集和架构上试试。你可以创建和训练你自己的模型，然后使用相同的方法欺骗它。你也可以看看随着你调整系数，预测的置信度epsilon如何变化\n",
    "\n",
    "尽管强大，本节展示的攻击只是对抗攻击研究的开端，有很多后面论文创建更强大的攻击。除了对抗攻击外，研究也导致防卫的产生，它的目的是创建强壮的机器学习模型。你可以参考 [survey paper](https://arxiv.org/abs/1810.00069)，全面了解对抗攻击和防卫。 \n",
    "\n",
    "对于更多的对抗攻击和方位的实现，你可以看看对抗样本库[CleverHans](https://github.com/tensorflow/cleverhans)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "adversarial_fgsm.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "py37tf2",
   "language": "python",
   "name": "py37tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
